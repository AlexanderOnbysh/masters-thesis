
@article{qi_pointnet_2017,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	url = {http://arxiv.org/abs/1612.00593},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	urldate = {2020-12-14},
	journal = {arXiv:1612.00593 [cs]},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	month = apr,
	year = {2017},
	note = {arXiv: 1612.00593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2017},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/QHXBL5U4/Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/UL7TNCEG/1612.html:text/html},
}

@article{zhao_3d_2019,
	title = {{3D} {Point} {Capsule} {Networks}},
	url = {http://arxiv.org/abs/1812.10775},
	abstract = {In this paper, we propose 3D point-capsule networks, an auto-encoder designed to process sparse 3D point clouds while preserving spatial arrangements of the input data. 3D capsule networks arise as a direct consequence of our novel unified 3D auto-encoder formulation. Their dynamic routing scheme and the peculiar 2D latent space deployed by our approach bring in improvements for several common point cloud-related tasks, such as object classification, object reconstruction and part segmentation as substantiated by our extensive evaluations. Moreover, it enables new applications such as part interpolation and replacement.},
	urldate = {2020-12-14},
	journal = {arXiv:1812.10775 [cs]},
	author = {Zhao, Yongheng and Birdal, Tolga and Deng, Haowen and Tombari, Federico},
	month = jul,
	year = {2019},
	note = {arXiv: 1812.10775},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: As published in CVPR 2019 (camera ready version), with supplementary material},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/GMHY945J/Zhao et al. - 2019 - 3D Point Capsule Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/UKAUHVQ4/1812.html:text/html},
}

@article{kakillioglu_3d_2020,
	title = {{3D} {Capsule} {Networks} for {Object} {Classification} {With} {Weight} {Pruning}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8984369/},
	doi = {10.1109/ACCESS.2020.2971950},
	urldate = {2020-12-14},
	journal = {IEEE Access},
	author = {Kakillioglu, Burak and Ren, Ao and Wang, Yanzhi and Velipasalar, Senem},
	year = {2020},
	pages = {27393--27405},
	file = {Full Text:/Users/alexon/Zotero/storage/5ICL8FQT/Kakillioglu et al. - 2020 - 3D Capsule Networks for Object Classification With.pdf:application/pdf},
}

@article{marusaki_capsule_2020,
	title = {Capsule {GAN} {Using} {Capsule} {Network} for {Generator} {Architecture}},
	url = {http://arxiv.org/abs/2003.08047},
	abstract = {This paper presents Capsule GAN, a Generative adversarial network using Capsule Network not only in the discriminator but also in the generator. Recently, Generative adversarial networks (GANs) has been intensively studied. However, generating images by GANs is difficult. Therefore, GANs sometimes generate poor quality images. These GANs use convolutional neural networks (CNNs). However, CNNs have the defect that the relational information between features of the image may be lost. Capsule Network, proposed by Hinton in 2017, overcomes the defect of CNNs. Capsule GAN reported previously uses Capsule Network in the discriminator. However, instead of using Capsule Network, Capsule GAN reported in previous studies uses CNNs in generator architecture like DCGAN. This paper introduces two approaches to use Capsule Network in the generator. One is to use DigitCaps layer from the discriminator as the input to the generator. DigitCaps layer is the output layer of Capsule Network. It has the features of the input images of the discriminator. The other is to use the reverse operation of recognition process in Capsule Network in the generator. We compare Capsule GAN proposed in this paper with conventional GAN using CNN and Capsule GAN which uses Capsule Network in the discriminator only. The datasets are MNIST, Fashion-MNIST and color images. We show that Capsule GAN outperforms the GAN using CNN and the GAN using Capsule Network in the discriminator only. The architecture of Capsule GAN proposed in this paper is a basic architecture using Capsule Network. Therefore, we can apply the existing improvement techniques for GANs to Capsule GAN.},
	urldate = {2020-12-14},
	journal = {arXiv:2003.08047 [cs, eess]},
	author = {Marusaki, Kanako and Watanabe, Hiroshi},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.08047},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, 68T05, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: 7 pages and 8 figures},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/ZZDE4TK8/Marusaki and Watanabe - 2020 - Capsule GAN Using Capsule Network for Generator Ar.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/IJSNY5FH/2003.html:text/html},
}

@article{wu_learning_2017,
	title = {Learning a {Probabilistic} {Latent} {Space} of {Object} {Shapes} via {3D} {Generative}-{Adversarial} {Modeling}},
	url = {http://arxiv.org/abs/1610.07584},
	abstract = {We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.},
	urldate = {2020-12-14},
	journal = {arXiv:1610.07584 [cs]},
	author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, William T. and Tenenbaum, Joshua B.},
	month = jan,
	year = {2017},
	note = {arXiv: 1610.07584},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: NIPS 2016. The first two authors contributed equally to this work},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/VJ94SR58/Wu et al. - 2017 - Learning a Probabilistic Latent Space of Object Sh.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/CEZU5KP5/1610.html:text/html},
}

@misc{noauthor_notitle_nodate,
}

@article{xue_general_2020,
	title = {A general generative adversarial capsule network for hyperspectral image spectral-spatial classification},
	volume = {11},
	issn = {2150-704X, 2150-7058},
	url = {https://www.tandfonline.com/doi/full/10.1080/2150704X.2019.1681598},
	doi = {10.1080/2150704X.2019.1681598},
	language = {en},
	number = {1},
	urldate = {2020-12-14},
	journal = {Remote Sensing Letters},
	author = {Xue, Zhixiang},
	month = jan,
	year = {2020},
	pages = {19--28},
}

@article{qin_detecting_2020,
	title = {Detecting and {Diagnosing} {Adversarial} {Images} with {Class}-{Conditional} {Capsule} {Reconstructions}},
	url = {http://arxiv.org/abs/1907.02957},
	abstract = {Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.},
	urldate = {2020-12-14},
	journal = {arXiv:1907.02957 [cs, stat]},
	author = {Qin, Yao and Frosst, Nicholas and Sabour, Sara and Raffel, Colin and Cottrell, Garrison and Hinton, Geoffrey},
	month = feb,
	year = {2020},
	note = {arXiv: 1907.02957},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/8ICAQG3S/Qin et al. - 2020 - Detecting and Diagnosing Adversarial Images with C.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/CNVWGCUI/1907.html:text/html},
}

@article{duarte_videocapsulenet_2018,
	title = {{VideoCapsuleNet}: {A} {Simplified} {Network} for {Action} {Detection}},
	shorttitle = {{VideoCapsuleNet}},
	url = {http://arxiv.org/abs/1805.08162},
	abstract = {The recent advances in Deep Convolutional Neural Networks (DCNNs) have shown extremely good results for video human action classification, however, action detection is still a challenging problem. The current action detection approaches follow a complex pipeline which involves multiple tasks such as tube proposals, optical flow, and tube classification. In this work, we present a more elegant solution for action detection based on the recently developed capsule network. We propose a 3D capsule network for videos, called VideoCapsuleNet: a unified network for action detection which can jointly perform pixel-wise action segmentation along with action classification. The proposed network is a generalization of capsule network from 2D to 3D, which takes a sequence of video frames as input. The 3D generalization drastically increases the number of capsules in the network, making capsule routing computationally expensive. We introduce capsule-pooling in the convolutional capsule layer to address this issue which makes the voting algorithm tractable. The routing-by-agreement in the network inherently models the action representations and various action characteristics are captured by the predicted capsules. This inspired us to utilize the capsules for action localization and the class-specific capsules predicted by the network are used to determine a pixel-wise localization of actions. The localization is further improved by parameterized skip connections with the convolutional capsule layers and the network is trained end-to-end with a classification as well as localization loss. The proposed network achieves sate-of-the-art performance on multiple action detection datasets including UCF-Sports, J-HMDB, and UCF-101 (24 classes) with an impressive {\textasciitilde}20\% improvement on UCF-101 and {\textasciitilde}15\% improvement on J-HMDB in terms of v-mAP scores.},
	urldate = {2020-12-14},
	journal = {arXiv:1805.08162 [cs]},
	author = {Duarte, Kevin and Rawat, Yogesh S. and Shah, Mubarak},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08162},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/MWB6DJGA/Duarte et al. - 2018 - VideoCapsuleNet A Simplified Network for Action D.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/JWFXWUAC/1805.html:text/html},
}

@article{lalonde_capsules_2018,
	title = {Capsules for {Object} {Segmentation}},
	url = {http://arxiv.org/abs/1804.04241},
	abstract = {Convolutional neural networks (CNNs) have shown remarkable results over the last several years for a wide range of computer vision tasks. A new architecture recently introduced by Sabour et al., referred to as a capsule networks with dynamic routing, has shown great initial results for digit recognition and small image classification. The success of capsule networks lies in their ability to preserve more information about the input by replacing max-pooling layers with convolutional strides and dynamic routing, allowing for preservation of part-whole relationships in the data. This preservation of the input is demonstrated by reconstructing the input from the output capsule vectors. Our work expands the use of capsule networks to the task of object segmentation for the first time in the literature. We extend the idea of convolutional capsules with locally-connected routing and propose the concept of deconvolutional capsules. Further, we extend the masked reconstruction to reconstruct the positive input class. The proposed convolutional-deconvolutional capsule network, called SegCaps, shows strong results for the task of object segmentation with substantial decrease in parameter space. As an example application, we applied the proposed SegCaps to segment pathological lungs from low dose CT scans and compared its accuracy and efficiency with other U-Net-based architectures. SegCaps is able to handle large image sizes (512 x 512) as opposed to baseline capsules (typically less than 32 x 32). The proposed SegCaps reduced the number of parameters of U-Net architecture by 95.4\% while still providing a better segmentation accuracy.},
	urldate = {2020-12-14},
	journal = {arXiv:1804.04241 [cs, stat]},
	author = {LaLonde, Rodney and Bagci, Ulas},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.04241},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/DTQVV3TH/LaLonde and Bagci - 2018 - Capsules for Object Segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/M8783DR9/1804.html:text/html},
}

@incollection{jawahar_multi-level_2019,
	address = {Cham},
	title = {Multi-level {Dense} {Capsule} {Networks}},
	volume = {11365},
	isbn = {978-3-030-20872-1 978-3-030-20873-8},
	url = {http://link.springer.com/10.1007/978-3-030-20873-8_37},
	language = {en},
	urldate = {2020-12-14},
	booktitle = {Computer {Vision} – {ACCV} 2018},
	publisher = {Springer International Publishing},
	author = {Phaye, Sai Samarth R. and Sikka, Apoorva and Dhall, Abhinav and Bathula, Deepti R.},
	editor = {Jawahar, C.V. and Li, Hongdong and Mori, Greg and Schindler, Konrad},
	year = {2019},
	doi = {10.1007/978-3-030-20873-8_37},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {577--592},
}

@article{sabour_dynamic_2017,
	title = {Dynamic {Routing} {Between} {Capsules}},
	url = {http://arxiv.org/abs/1710.09829},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	urldate = {2020-12-14},
	journal = {arXiv:1710.09829 [cs]},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
	month = nov,
	year = {2017},
	note = {arXiv: 1710.09829},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/6656ZM2R/Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/FXM582RQ/1710.html:text/html},
}

@inproceedings{zhang_graph-cnn_2018,
	address = {Calgary, AB},
	title = {A {Graph}-{CNN} for {3D} {Point} {Cloud} {Classification}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8462291/},
	doi = {10.1109/ICASSP.2018.8462291},
	urldate = {2020-12-14},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Zhang, Yingxue and Rabbat, Michael},
	month = apr,
	year = {2018},
	pages = {6279--6283},
	file = {Submitted Version:/Users/alexon/Zotero/storage/ZJR6JRLF/Zhang and Rabbat - 2018 - A Graph-CNN for 3D Point Cloud Classification.pdf:application/pdf},
}

@article{qi_pointnet_2017-1,
	title = {{PointNet}++: {Deep} {Hierarchical} {Feature} {Learning} on {Point} {Sets} in a {Metric} {Space}},
	shorttitle = {{PointNet}++},
	url = {http://arxiv.org/abs/1706.02413},
	abstract = {Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	urldate = {2020-12-14},
	journal = {arXiv:1706.02413 [cs]},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02413},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/8WHV566L/Qi et al. - 2017 - PointNet++ Deep Hierarchical Feature Learning on .pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/EPQELBDD/1706.html:text/html},
}

@article{wang_dynamic_2019,
	title = {Dynamic {Graph} {CNN} for {Learning} on {Point} {Clouds}},
	volume = {38},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3326362},
	doi = {10.1145/3326362},
	language = {en},
	number = {5},
	urldate = {2020-12-14},
	journal = {ACM Transactions on Graphics},
	author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
	month = nov,
	year = {2019},
	pages = {1--12},
	file = {Full Text:/Users/alexon/Zotero/storage/ISZW38SR/Wang et al. - 2019 - Dynamic Graph CNN for Learning on Point Clouds.pdf:application/pdf},
}

@article{xu_geometry_2020,
	title = {Geometry {Sharing} {Network} for {3D} {Point} {Cloud} {Classification} and {Segmentation}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/6938},
	doi = {10.1609/aaai.v34i07.6938},
	abstract = {In spite of the recent progresses on classifying 3D point cloud with deep CNNs, large geometric transformations like rotation and translation remain challenging problem and harm the final classification performance. To address this challenge, we propose Geometry Sharing Network (GS-Net) which effectively learns point descriptors with holistic context to enhance the robustness to geometric transformations. Compared with previous 3D point CNNs which perform convolution on nearby points, GS-Net can aggregate point features in a more global way. Specially, GS-Net consists of Geometry Similarity Connection (GSC) modules which exploit Eigen-Graph to group distant points with similar and relevant geometric information, and aggregate features from nearest neighbors in both Euclidean space and Eigenvalue space. This design allows GS-Net to efficiently capture both local and holistic geometric features such as symmetry, curvature, convexity and connectivity. Theoretically, we show the nearest neighbors of each point in Eigenvalue space are invariant to rotation and translation. We conduct extensive experiments on public datasets, ModelNet40, ShapeNet Part. Experiments demonstrate that GS-Net achieves the state-of-the-art performances on major datasets, 93.3\% on ModelNet40, and are more robust to geometric transformations.},
	number = {07},
	urldate = {2020-12-14},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Xu, Mingye and Zhou, Zhipeng and Qiao, Yu},
	month = apr,
	year = {2020},
	pages = {12500--12507},
	file = {Full Text:/Users/alexon/Zotero/storage/MRV5BRN8/Xu et al. - 2020 - Geometry Sharing Network for 3D Point Cloud Classi.pdf:application/pdf},
}

@article{bahadori_spectral_2018,
	title = {{SPECTRAL} {CAPSULE} {NETWORKS}},
	abstract = {In search for more accurate predictive models, we customize capsule networks for the learning to diagnose problem. We also propose Spectral Capsule Networks, a novel variation of capsule networks, that converge faster than capsule network with EM routing. Spectral capsule networks consist of spatial coincidence ﬁlters that detect entities based on the alignment of extracted features on a one-dimensional linear subspace. Experiments on a public benchmark learning to diagnose dataset not only shows the success of capsule networks on this task, but also conﬁrm the faster convergence of the spectral capsule networks.},
	language = {en},
	author = {Bahadori, Mohammad Taha},
	year = {2018},
	pages = {5},
	file = {Bahadori - 2018 - SPECTRAL CAPSULE NETWORKS.pdf:/Users/alexon/Zotero/storage/B9J3YCWG/Bahadori - 2018 - SPECTRAL CAPSULE NETWORKS.pdf:application/pdf},
}

@article{wang_optimization_2018,
	title = {{AN} {OPTIMIZATION} {VIEW} {ON} {DYNAMIC} {ROUTING} {BE}- {TWEEN} {CAPSULES}},
	abstract = {Despite the effectiveness of dynamic routing procedure recently proposed in (Sabour et al., 2017), we still lack a standard formalization of the heuristic and its implications. In this paper, we partially formulate the routing strategy proposed in Sabour et al. (2017) as an optimization problem that minimizes a combination of clustering-like loss and a KL regularization term between the current coupling distribution and its last states. We then introduce another simple routing approach, which enjoys few interesting properties. In an unsupervised perceptual grouping task, we show experimentally that our routing algorithm outperforms the dynamic routing method proposed in Sabour et al. (2017).},
	language = {en},
	author = {Wang, Dilin and Liu, Qiang},
	year = {2018},
	pages = {4},
	file = {Wang and Liu - 2018 - AN OPTIMIZATION VIEW ON DYNAMIC ROUTING BE- TWEEN .pdf:/Users/alexon/Zotero/storage/VG37W37L/Wang and Liu - 2018 - AN OPTIMIZATION VIEW ON DYNAMIC ROUTING BE- TWEEN .pdf:application/pdf},
}

@article{bello_review_2020,
	title = {Review: deep learning on {3D} point clouds},
	shorttitle = {Review},
	url = {http://arxiv.org/abs/2001.06280},
	abstract = {Point cloud is point sets defined in 3D metric space. Point cloud has become one of the most significant data format for 3D representation. Its gaining increased popularity as a result of increased availability of acquisition devices, such as LiDAR, as well as increased application in areas such as robotics, autonomous driving, augmented and virtual reality. Deep learning is now the most powerful tool for data processing in computer vision, becoming the most preferred technique for tasks such as classification, segmentation, and detection. While deep learning techniques are mainly applied to data with a structured grid, point cloud, on the other hand, is unstructured. The unstructuredness of point clouds makes use of deep learning for its processing directly very challenging. Earlier approaches overcome this challenge by preprocessing the point cloud into a structured grid format at the cost of increased computational cost or lost of depth information. Recently, however, many state-of-the-arts deep learning techniques that directly operate on point cloud are being developed. This paper contains a survey of the recent state-of-the-art deep learning techniques that mainly focused on point cloud data. We first briefly discussed the major challenges faced when using deep learning directly on point cloud, we also briefly discussed earlier approaches which overcome the challenges by preprocessing the point cloud into a structured grid. We then give the review of the various state-of-the-art deep learning approaches that directly process point cloud in its unstructured form. We introduced the popular 3D point cloud benchmark datasets. And we also further discussed the application of deep learning in popular 3D vision tasks including classification, segmentation and detection.},
	urldate = {2020-12-14},
	journal = {arXiv:2001.06280 [cs]},
	author = {Bello, Saifullahi Aminu and Yu, Shangshu and Wang, Cheng},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.06280},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/569QRJNT/Bello et al. - 2020 - Review deep learning on 3D point clouds.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/LBTUTFJF/2001.html:text/html},
}

@inproceedings{zhang_unsupervised_2019,
	address = {Québec City, QC, Canada},
	title = {Unsupervised {Feature} {Learning} for {Point} {Cloud} {Understanding} by {Contrasting} and {Clustering} {Using} {Graph} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-72813-131-3},
	url = {https://ieeexplore.ieee.org/document/8885536/},
	doi = {10.1109/3DV.2019.00051},
	urldate = {2020-12-14},
	booktitle = {2019 {International} {Conference} on {3D} {Vision} ({3DV})},
	publisher = {IEEE},
	author = {Zhang, Ling and Zhu, Zhigang},
	month = sep,
	year = {2019},
	pages = {395--404},
}

@article{afshar_3d-mcn_2020,
	title = {{3D}-{MCN}: {A} {3D} {Multi}-scale {Capsule} {Network} for {Lung} {Nodule} {Malignancy} {Prediction}},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	shorttitle = {{3D}-{MCN}},
	url = {https://www.nature.com/articles/s41598-020-64824-5},
	doi = {10.1038/s41598-020-64824-5},
	abstract = {Despite the advances in automatic lung cancer malignancy prediction, achieving high accuracy remains challenging. Existing solutions are mostly based on Convolutional Neural Networks (CNNs), which require a large amount of training data. Most of the developed CNN models are based only on the main nodule region, without considering the surrounding tissues. Obtaining high sensitivity is challenging with lung nodule malignancy prediction. Moreover, the interpretability of the proposed techniques should be a consideration when the end goal is to utilize the model in a clinical setting. Capsule networks (CapsNets) are new and revolutionary machine learning architectures proposed to overcome shortcomings of CNNs. Capitalizing on the success of CapsNet in biomedical domains, we propose a novel model for lung tumor malignancy prediction. The proposed framework, referred to as the 3D Multi-scale Capsule Network (3D-MCN), is uniquely designed to benefit from: (i) 3D inputs, providing information about the nodule in 3D; (ii) Multi-scale input, capturing the nodule’s local features, as well as the characteristics of the surrounding tissues, and; (iii) CapsNet-based design, being capable of dealing with a small number of training samples. The proposed 3D—MCN architecture predicted lung nodule malignancy with a high accuracy of 93.12\%, sensitivity of 94.94\%, area under the curve (AUC) of 0.9641, and specificity of 90\% when tested on the LIDC-IDRI dataset. When classifying patients as having a malignant condition (i.e., at least one malignant nodule is detected) or not, the proposed model achieved an accuracy of 83\%, and a sensitivity and specificity of 84\% and 81\% respectively.},
	language = {en},
	number = {1},
	urldate = {2020-12-14},
	journal = {Scientific Reports},
	author = {Afshar, Parnian and Oikonomou, Anastasia and Naderkhani, Farnoosh and Tyrrell, Pascal N. and Plataniotis, Konstantinos N. and Farahani, Keyvan and Mohammadi, Arash},
	month = may,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {7948},
	file = {Full Text PDF:/Users/alexon/Zotero/storage/JJ9Q64Y8/Afshar et al. - 2020 - 3D-MCN A 3D Multi-scale Capsule Network for Lung .pdf:application/pdf;Snapshot:/Users/alexon/Zotero/storage/IVGAFBCT/s41598-020-64824-5.html:text/html},
}

@inproceedings{cheraghian_3dcapsule_2019,
	title = {{3DCapsule}: {Extending} the {Capsule} {Architecture} to {Classify} {3D} {Point} {Clouds}},
	shorttitle = {{3DCapsule}},
	doi = {10.1109/WACV.2019.00132},
	abstract = {This paper introduces the 3DCapsule, which is a 3D extension of the recently introduced Capsule concept that makes it applicable to unordered point sets. The original Capsule relies on the existence of a spatial relationship between the elements in the feature map it is presented with, whereas in point permutation invariant formulations of 3D point set classification methods, such relationships are typically lost. Here, a new layer called ComposeCaps is introduced that, in lieu of a spatially relevant feature mapping, learns a new mapping that can be exploited by the 3DCapsule. Previous works in the 3D point set classification domain have focused on other parts of the architecture, whereas instead, the 3DCapsule is a drop-in replacement of the commonly used fully connected classifier. It is demonstrated via an ablation study, that when the 3DCapsule is applied to recent 3D point set classification architectures, it consistently shows an improvement, in particular when subjected to noisy data. Similarly, the ComposeCaps layer is evaluated and demonstrates an improvement over the baseline. In an apples-to-apples comparison against state-of-the-art methods, again, better performance is demonstrated by the 3DCapsule.},
	booktitle = {2019 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Cheraghian, A. and Petersson, L.},
	month = jan,
	year = {2019},
	note = {ISSN: 1550-5790},
	keywords = {3D point clouds, 3D point set classification domain, 3DCapsule, Computer architecture, Deep learning, feature extraction, Feature extraction, Heuristic algorithms, image classification, learning (artificial intelligence), pattern classification, point permutation invariant formulations, Routing, Three-dimensional displays, Two dimensional displays, unordered point sets},
	pages = {1194--1202},
	file = {IEEE Xplore Abstract Record:/Users/alexon/Zotero/storage/JEC5U9FZ/8658405.html:text/html},
}

@article{cheraghian_3dcapsule_2018,
	title = {{3DCapsule}: {Extending} the {Capsule} {Architecture} to {Classify} {3D} {Point} {Clouds}},
	shorttitle = {{3DCapsule}},
	url = {http://arxiv.org/abs/1811.02191},
	abstract = {This paper introduces the 3DCapsule, which is a 3D extension of the recently introduced Capsule concept that makes it applicable to unordered point sets. The original Capsule relies on the existence of a spatial relationship between the elements in the feature map it is presented with, whereas in point permutation invariant formulations of 3D point set classification methods, such relationships are typically lost. Here, a new layer called ComposeCaps is introduced that, in lieu of a spatially relevant feature mapping, learns a new mapping that can be exploited by the 3DCapsule. Previous works in the 3D point set classification domain have focused on other parts of the architecture, whereas instead, the 3DCapsule is a drop-in replacement of the commonly used fully connected classifier. It is demonstrated via an ablation study, that when the 3DCapsule is applied to recent 3D point set classification architectures, it consistently shows an improvement, in particular when subjected to noisy data. Similarly, the ComposeCaps layer is evaluated and demonstrates an improvement over the baseline. In an apples-to-apples comparison against state-of-the-art methods, again, better performance is demonstrated by the 3DCapsule.},
	urldate = {2020-12-14},
	journal = {arXiv:1811.02191 [cs]},
	author = {Cheraghian, Ali and Petersson, Lars},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.02191},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/MHGF98LU/Cheraghian and Petersson - 2018 - 3DCapsule Extending the Capsule Architecture to C.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/DC2FMQN3/1811.html:text/html},
}

@misc{rovai_realtime_2020,
	title = {Realtime {Multiple} {Person} {2D} {Pose} {Estimation} using {TensorFlow2}.x},
	url = {https://towardsdatascience.com/realtime-multiple-person-2d-pose-estimation-using-tensorflow2-x-93e4c156d45f},
	abstract = {Pose estimation is crucial in enabling machines to understand people in images and videos.},
	language = {en},
	urldate = {2020-12-25},
	journal = {Medium},
	author = {Rovai, Marcelo},
	month = aug,
	year = {2020},
	file = {Snapshot:/Users/alexon/Zotero/storage/SLUYS296/realtime-multiple-person-2d-pose-estimation-using-tensorflow2-x-93e4c156d45f.html:text/html},
}

@article{wu_3d_2020,
	title = {{3D} {Capsule} {Hand} {Pose} {Estimation} {Network} {Based} on {Structural} {Relationship} {Information}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2073-8994/12/10/1636},
	doi = {10.3390/sym12101636},
	abstract = {Hand pose estimation from 3D data is a key challenge in computer vision as well as an essential step for human\&ndash;computer interaction. A lot of deep learning-based hand pose estimation methods have made significant progress but give less consideration to the inner interactions of input data, especially when consuming hand point clouds. Therefore, this paper proposes an end-to-end capsule-based hand pose estimation network (Capsule-HandNet), which processes hand point clouds directly with the consideration of structural relationships among local parts, including symmetry, junction, relative location, etc. Firstly, an encoder is adopted in Capsule-HandNet to extract multi-level features into the latent capsule by dynamic routing. The latent capsule represents the structural relationship information of the hand point cloud explicitly. Then, a decoder recovers a point cloud to fit the input hand point cloud via a latent capsule. This auto-encoder procedure is designed to ensure the effectiveness of the latent capsule. Finally, the hand pose is regressed from the combined feature, which consists of the global feature and the latent capsule. The Capsule-HandNet is evaluated on public hand pose datasets under the metrics of the mean error and the fraction of frames. The mean joint errors of Capsule-HandNet on MSRA and ICVL datasets reach 8.85 mm and 7.49 mm, respectively, and Capsule-HandNet outperforms the state-of-the-art methods on most thresholds under the fraction of frames metric. The experimental results demonstrate the effectiveness of Capsule-HandNet for 3D hand pose estimation.},
	language = {en},
	number = {10},
	urldate = {2020-12-25},
	journal = {Symmetry},
	author = {Wu, Yiqi and Ma, Shichao and Zhang, Dejun and Sun, Jun},
	month = oct,
	year = {2020},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {capsule, deep neural network, hand pose estimation, point cloud, structural relationship},
	pages = {1636},
	file = {Full Text PDF:/Users/alexon/Zotero/storage/JM58VN9D/Wu et al. - 2020 - 3D Capsule Hand Pose Estimation Network Based on S.pdf:application/pdf},
}

@inproceedings{su_multi-view_2015,
	address = {Santiago, Chile},
	title = {Multi-view {Convolutional} {Neural} {Networks} for {3D} {Shape} {Recognition}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410471/},
	doi = {10.1109/ICCV.2015.114},
	abstract = {A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We ﬁrst present a standard CNN architecture trained to recognize the shapes’ rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
	month = dec,
	year = {2015},
	pages = {945--953},
	file = {Su et al. - 2015 - Multi-view Convolutional Neural Networks for 3D Sh.pdf:/Users/alexon/Zotero/storage/LDCJTRV4/Su et al. - 2015 - Multi-view Convolutional Neural Networks for 3D Sh.pdf:application/pdf},
}

@inproceedings{wei_view-gcn_2020,
	address = {Seattle, WA, USA},
	title = {View-{GCN}: {View}-{Based} {Graph} {Convolutional} {Network} for {3D} {Shape} {Analysis}},
	isbn = {978-1-72817-168-5},
	shorttitle = {View-{GCN}},
	url = {https://ieeexplore.ieee.org/document/9156567/},
	doi = {10.1109/CVPR42600.2020.00192},
	abstract = {View-based approach that recognizes 3D shape through its projected 2D images has achieved state-of-the-art results for 3D shape recognition. The major challenge for view-based approach is how to aggregate multi-view features to be a global shape descriptor. In this work, we propose a novel view-based Graph Convolutional Neural Network, dubbed as view-GCN, to recognize 3D shape based on graph representation of multiple views in ﬂexible view conﬁgurations. We ﬁrst construct view-graph with multiple views as graph nodes, then design a graph convolutional neural network over view-graph to hierarchically learn discriminative shape descriptor considering relations of multiple views. The view-GCN is a hierarchical network based on local and non-local graph convolution for feature transform, and selective view-sampling for graph coarsening. Extensive experiments on benchmark datasets show that view-GCN achieves state-of-the-art results for 3D shape classiﬁcation and retrieval.},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wei, Xin and Yu, Ruixuan and Sun, Jian},
	month = jun,
	year = {2020},
	pages = {1847--1856},
	file = {Wei et al. - 2020 - View-GCN View-Based Graph Convolutional Network f.pdf:/Users/alexon/Zotero/storage/K4L9X5SS/Wei et al. - 2020 - View-GCN View-Based Graph Convolutional Network f.pdf:application/pdf},
}

@article{pishchulin_deepcut_2016,
	title = {{DeepCut}: {Joint} {Subset} {Partition} and {Labeling} for {Multi} {Person} {Pose} {Estimation}},
	shorttitle = {{DeepCut}},
	url = {http://arxiv.org/abs/1511.06645},
	abstract = {This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation. Models and code available at http://pose.mpi-inf.mpg.de.},
	urldate = {2020-12-25},
	journal = {arXiv:1511.06645 [cs]},
	author = {Pishchulin, Leonid and Insafutdinov, Eldar and Tang, Siyu and Andres, Bjoern and Andriluka, Mykhaylo and Gehler, Peter and Schiele, Bernt},
	month = apr,
	year = {2016},
	note = {arXiv: 1511.06645},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted at IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/D55UDMIM/Pishchulin et al. - 2016 - DeepCut Joint Subset Partition and Labeling for M.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/VP2DS4HH/1511.html:text/html},
}

@article{cao_openpose_2019,
	title = {{OpenPose}: {Realtime} {Multi}-{Person} {2D} {Pose} {Estimation} using {Part} {Affinity} {Fields}},
	shorttitle = {{OpenPose}},
	url = {http://arxiv.org/abs/1812.08008},
	abstract = {Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.},
	urldate = {2020-12-25},
	journal = {arXiv:1812.08008 [cs]},
	author = {Cao, Zhe and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
	month = may,
	year = {2019},
	note = {arXiv: 1812.08008},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Journal version of arXiv:1611.08050, with better accuracy and faster speed, release a new foot keypoint dataset: https://cmu-perceptual-computing-lab.github.io/foot\_keypoint\_dataset/},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/XCZUGMFB/Cao et al. - 2019 - OpenPose Realtime Multi-Person 2D Pose Estimation.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/4G2ECU2Z/1812.html:text/html},
}

@article{fang_rmpe_2018,
	title = {{RMPE}: {Regional} {Multi}-person {Pose} {Estimation}},
	shorttitle = {{RMPE}},
	url = {http://arxiv.org/abs/1612.00137},
	abstract = {Multi-person pose estimation in the wild is challenging. Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable. These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results. In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve a 17\% increase in mAP over the state-of-the-art methods on the MPII (multi person) dataset.Our model and source codes are publicly available.},
	urldate = {2020-12-25},
	journal = {arXiv:1612.00137 [cs]},
	author = {Fang, Hao-Shu and Xie, Shuqin and Tai, Yu-Wing and Lu, Cewu},
	month = feb,
	year = {2018},
	note = {arXiv: 1612.00137},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Models \& Codes available at https://github.com/MVIG-SJTU/RMPE or https://github.com/Fang-Haoshu/RMPE},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/6E69SIY5/Fang et al. - 2018 - RMPE Regional Multi-person Pose Estimation.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/3DSDCGEN/1612.html:text/html},
}

@article{lu_monet_2020,
	title = {{MoNet}: {Motion}-based {Point} {Cloud} {Prediction} {Network}},
	shorttitle = {{MoNet}},
	url = {http://arxiv.org/abs/2011.10812},
	abstract = {Predicting the future can significantly improve the safety of intelligent vehicles, which is a key component in autonomous driving. 3D point clouds accurately model 3D information of surrounding environment and are crucial for intelligent vehicles to perceive the scene. Therefore, prediction of 3D point clouds has great significance for intelligent vehicles, which can be utilized for numerous further applications. However, due to point clouds are unordered and unstructured, point cloud prediction is challenging and has not been deeply explored in current literature. In this paper, we propose a novel motion-based neural network named MoNet. The key idea of the proposed MoNet is to integrate motion features between two consecutive point clouds into the prediction pipeline. The introduction of motion features enables the model to more accurately capture the variations of motion information across frames and thus make better predictions for future motion. In addition, content features are introduced to model the spatial content of individual point clouds. A recurrent neural network named MotionRNN is proposed to capture the temporal correlations of both features. Besides, we propose an attention-based motion align module to address the problem of missing motion features in the inference pipeline. Extensive experiments on two large scale outdoor LiDAR datasets demonstrate the performance of the proposed MoNet. Moreover, we perform experiments on applications using the predicted point clouds and the results indicate the great application potential of the proposed method.},
	urldate = {2020-12-25},
	journal = {arXiv:2011.10812 [cs]},
	author = {Lu, Fan and Chen, Guang and Liu, Yinlong and Li, Zhijun and Qu, Sanqing and Zou, Tianpei},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.10812},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/VUU6XRZ7/Lu et al. - 2020 - MoNet Motion-based Point Cloud Prediction Network.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/VBT3QYJS/2011.html:text/html},
}

@article{yang_modeling_2019,
	title = {Modeling {Point} {Clouds} with {Self}-{Attention} and {Gumbel} {Subset} {Sampling}},
	url = {http://arxiv.org/abs/1904.03375},
	abstract = {Geometric deep learning is increasingly important thanks to the popularity of 3D sensors. Inspired by the recent advances in NLP domain, the self-attention transformer is introduced to consume the point clouds. We develop Point Attention Transformers (PATs), using a parameter-efficient Group Shuffle Attention (GSA) to replace the costly Multi-Head Attention. We demonstrate its ability to process size-varying inputs, and prove its permutation equivariance. Besides, prior work uses heuristics dependence on the input data (e.g., Furthest Point Sampling) to hierarchically select subsets of input points. Thereby, we for the first time propose an end-to-end learnable and task-agnostic sampling operation, named Gumbel Subset Sampling (GSS), to select a representative subset of input points. Equipped with Gumbel-Softmax, it produces a "soft" continuous subset in training phase, and a "hard" discrete subset in test phase. By selecting representative subsets in a hierarchical fashion, the networks learn a stronger representation of the input sets with lower computation cost. Experiments on classification and segmentation benchmarks show the effectiveness and efficiency of our methods. Furthermore, we propose a novel application, to process event camera stream as point clouds, and achieve a state-of-the-art performance on DVS128 Gesture Dataset.},
	urldate = {2020-12-25},
	journal = {arXiv:1904.03375 [cs]},
	author = {Yang, Jiancheng and Zhang, Qiang and Ni, Bingbing and Li, Linguo and Liu, Jinxian and Zhou, Mengdie and Tian, Qi},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.03375},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: CVPR'2019},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/4RMI2DNP/Yang et al. - 2019 - Modeling Point Clouds with Self-Attention and Gumb.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/HEEUY5B2/1904.html:text/html},
}

@article{zhou_learning_2020,
	title = {Learning to {Estimate} {3D} {Human} {Pose} {From} {Point} {Cloud}},
	volume = {20},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2020.2999849},
	abstract = {3D pose estimation is a challenging problem in computer vision. Most of the existing neural-network-based approaches address color or depth images through convolution networks (CNNs). In this paper, we study the task of 3D human pose estimation from depth images. Different from the existing CNN-based human pose estimation method, we propose a deep human pose network for 3D pose estimation by taking the point cloud data as input data to model the surface of complex human structures. We first cast the 3D human pose estimation from 2D depth images to 3D point clouds and directly predict the 3D joint position. Our experiments on two public datasets show that our approach achieves higher accuracy than previous state-of-art methods. The reported results on both ITOP and EVAL datasets demonstrate the effectiveness of our method on the targeted tasks.},
	number = {20},
	journal = {IEEE Sensors Journal},
	author = {Zhou, Y. and Dong, H. and Saddik, A. E.},
	month = oct,
	year = {2020},
	note = {Conference Name: IEEE Sensors Journal},
	keywords = {3D human pose estimation, 3D joint position, 3D point clouds, 3D pose estimation, Cameras, complex human structures, computer vision, convolutional neural nets, deep human, Deep learning, depth image, depth images, Edge feature, estimation method, existing neural-network-based approaches, Feature extraction, learning (artificial intelligence), point cloud data, pose estimation, Pose estimation, pose regression network, Sensors, Solid modeling, Three-dimensional displays},
	pages = {12334--12342},
	file = {IEEE Xplore Abstract Record:/Users/alexon/Zotero/storage/JZ88Z648/9107101.html:text/html},
}

@inproceedings{diaz_barros_real-time_2015,
	title = {Real-{Time} {Human} {Pose} {Estimation} from {Body}-{Scanned} {Point} {Clouds}},
	volume = {1},
	doi = {10.5220/0005309005530560},
	abstract = {This paper presents a novel approach to estimate the human pose from a body-scanned point cloud. To do so, a predefined skeleton model is first initialized according to both the skeleton base point and its torso limb obtained by Principal Component Analysis (PCA). Then, the body parts are iteratively clustered and the skeleton limb fitting is performed, based on Expectation Maximization (EM). The human pose is given by the location of each skeletal node in the fitted skeleton model. Experimental results show the ability of the method to estimate the human pose from multiple point cloud video sequences representing the external surface of a scanned human body; being robust, precise and handling large portions of missing data due to occlusions, acquisition hindrances or registration inaccuracies.},
	author = {Diaz Barros, Jilliam Maria and Garcia, Frederic and Sidibé, Désiré},
	month = mar,
	year = {2015},
	file = {Full Text PDF:/Users/alexon/Zotero/storage/7PQZFYVF/Diaz Barros et al. - 2015 - Real-Time Human Pose Estimation from Body-Scanned .pdf:application/pdf},
}

@article{chan_3d_2016,
	title = {On the {3D} point cloud for human-pose estimation},
	url = {https://docs.lib.purdue.edu/open_access_dissertations/630},
	journal = {Open Access Dissertations},
	author = {Chan, Kai-Chi},
	month = apr,
	year = {2016},
	file = {:/Users/alexon/Zotero/storage/8HFT3KS7/630.html:text/html},
}

@article{marin-jimenez_3d_2018,
	title = {{3D} human pose estimation from depth maps using a deep combination of poses},
	url = {http://arxiv.org/abs/1807.05389},
	abstract = {Many real-world applications require the estimation of human body joints for higher-level tasks as, for example, human behaviour understanding. In recent years, depth sensors have become a popular approach to obtain three-dimensional information. The depth maps generated by these sensors provide information that can be employed to disambiguate the poses observed in two-dimensional images. This work addresses the problem of 3D human pose estimation from depth maps employing a Deep Learning approach. We propose a model, named Deep Depth Pose (DDP), which receives a depth map containing a person and a set of predefined 3D prototype poses and returns the 3D position of the body joints of the person. In particular, DDP is defined as a ConvNet that computes the specific weights needed to linearly combine the prototypes for the given input. We have thoroughly evaluated DDP on the challenging 'ITOP' and 'UBC3V' datasets, which respectively depict realistic and synthetic samples, defining a new state-of-the-art on them.},
	urldate = {2020-12-25},
	journal = {arXiv:1807.05389 [cs]},
	author = {Marin-Jimenez, Manuel J. and Romero-Ramirez, Francisco J. and Muñoz-Salinas, Rafael and Medina-Carnicer, Rafael},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.05389},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction},
	annote = {Comment: Accepted for publication at "Journal of Visual Communication and Image Representation"},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/MHBVCNWS/Marin-Jimenez et al. - 2018 - 3D human pose estimation from depth maps using a d.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/G4H3HFXK/1807.html:text/html},
}

@inproceedings{yu_multi-view_2018,
	address = {Salt Lake City, UT},
	title = {Multi-view {Harmonized} {Bilinear} {Network} for {3D} {Object} {Recognition}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578125/},
	doi = {10.1109/CVPR.2018.00027},
	abstract = {View-based methods have achieved considerable success in 3D object recognition tasks. Different from existing viewbased methods pooling the view-wise features, we tackle this problem from the perspective of patches-to-patches similarity measurement. By exploiting the relationship between polynomial kernel and bilinear pooling, we obtain an effective 3D object representation by aggregating local convolutional features through bilinear pooling. Meanwhile, we harmonize different components inherited in the bilinear feature to obtain a more discriminative representation. To achieve an end-to-end trainable framework, we incorporate the harmonized bilinear pooling as a layer of a network, constituting the proposed Multi-view Harmonized Bilinear Network (MHBN). Systematic experiments conducted on two public benchmark datasets demonstrate the efﬁcacy of the proposed methods in 3D object recognition.},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yu, Tan and Meng, Jingjing and Yuan, Junsong},
	month = jun,
	year = {2018},
	pages = {186--194},
	file = {Yu et al. - 2018 - Multi-view Harmonized Bilinear Network for 3D Obje.pdf:/Users/alexon/Zotero/storage/BDEYWXU8/Yu et al. - 2018 - Multi-view Harmonized Bilinear Network for 3D Obje.pdf:application/pdf},
}

@inproceedings{maturana_voxnet_2015,
	title = {{VoxNet}: {A} {3D} {Convolutional} {Neural} {Network} for real-time object recognition},
	shorttitle = {{VoxNet}},
	doi = {10.1109/IROS.2015.7353481},
	abstract = {Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Maturana, D. and Scherer, S.},
	month = sep,
	year = {2015},
	keywords = {3D convolutional neural network, Feature extraction, image representation, Laser radar, Neural networks, neurocontrollers, object recognition, Object recognition, point cloud data, range sensors, real-time object recognition, real-time systems, robot vision, robots, Robots, robust object recognition, Sensors, Three-dimensional displays, volumetric occupancy grid representation, VoxNet},
	pages = {922--928},
	file = {IEEE Xplore Abstract Record:/Users/alexon/Zotero/storage/BC78WS8Z/7353481.html:text/html},
}

@article{riegler_octnet_2017,
	title = {{OctNet}: {Learning} {Deep} {3D} {Representations} at {High} {Resolutions}},
	shorttitle = {{OctNet}},
	url = {http://arxiv.org/abs/1611.05009},
	abstract = {We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.},
	urldate = {2020-12-25},
	journal = {arXiv:1611.05009 [cs]},
	author = {Riegler, Gernot and Ulusoy, Ali Osman and Geiger, Andreas},
	month = apr,
	year = {2017},
	note = {arXiv: 1611.05009},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2017 camera ready},
	file = {arXiv Fulltext PDF:/Users/alexon/Zotero/storage/48UICGDY/Riegler et al. - 2017 - OctNet Learning Deep 3D Representations at High R.pdf:application/pdf;arXiv.org Snapshot:/Users/alexon/Zotero/storage/CML6RFWQ/1611.html:text/html},
}

@article{wang_o-cnn_2017,
	title = {O-{CNN}: octree-based convolutional neural networks for {3D} shape analysis},
	volume = {36},
	issn = {0730-0301, 1557-7368},
	shorttitle = {O-{CNN}},
	url = {https://dl.acm.org/doi/10.1145/3072959.3073608},
	doi = {10.1145/3072959.3073608},
	language = {en},
	number = {4},
	urldate = {2020-12-25},
	journal = {ACM Transactions on Graphics},
	author = {Wang, Peng-Shuai and Liu, Yang and Guo, Yu-Xiao and Sun, Chun-Yu and Tong, Xin},
	month = jul,
	year = {2017},
	pages = {1--11},
	file = {Wang et al. - 2017 - O-CNN octree-based convolutional neural networks .pdf:/Users/alexon/Zotero/storage/GCMCPBYC/Wang et al. - 2017 - O-CNN octree-based convolutional neural networks .pdf:application/pdf},
}
